{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Pl8-UW965GVW877UKXN0KZQuEmt5ePUU",
      "authorship_tag": "ABX9TyNVa6pgw5MKn0W54QCeZl3n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/melkatewabe10/Machine-learning_LST-Estimation-/blob/main/RF_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Author: Tewabe Melkamu**\n",
        "\n",
        "Date: 3/14/2025\n",
        "\n",
        "RF_model"
      ],
      "metadata": {
        "id": "xU8_qhBJmhVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install rasterio"
      ],
      "metadata": {
        "id": "35ixFHLYrJzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import necessary libraries**"
      ],
      "metadata": {
        "id": "kki7hkVIvYsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Author: Tewabe Melkamu\n",
        "#Date: 3/14/2025\n",
        "#RF_model\n",
        "\"\"\"\n",
        "Script to predict Land Surface Temperature (LST) using Random Forest Regression.\n",
        "This script is tailored for Google Colab. It loads GeoTIFF training data from Google Drive,\n",
        "preprocesses the data, trains the model (ntree=200, mtry=4),\n",
        "evaluates model performance (OOB, RMSE, MAE, R2), and predicts the entire image.\n",
        "Additional code sections provide data visualizations and save evaluation metrics.\n",
        "\"\"\"\n",
        "\n",
        "# =============================================================================\n",
        "# 1. Import Necessary Libraries\n",
        "# =============================================================================\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import rasterio\n",
        "import matplotlib.pyplot as plt\n",
        "from rasterio.plot import show\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Additional libraries for visualization and file operations\n",
        "from collections import OrderedDict\n"
      ],
      "metadata": {
        "id": "i41UHJ-kglKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Data Folder and File Paths**"
      ],
      "metadata": {
        "id": "PWeGTGRlvoLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Define Data Folder and File Paths\n",
        "# =============================================================================\n",
        "# Update the folder path below to the directory where your .tif files reside on your Drive.\n",
        "data_folder = '/content/drive/MyDrive/Taiwan_Yearly /Taiwan_2024'\n",
        "\n",
        "# Create a dictionary to store file paths for each predictor variable.\n",
        "predictor_files = OrderedDict({\n",
        "    'NDVI':   os.path.join(data_folder, 'NDVI_2024.tif'),\n",
        "    'EVI': os.path.join(data_folder, 'EVI_2024.tif'),\n",
        "    'NDWI':  os.path.join(data_folder, 'NDWI_2024.tif'),\n",
        "    'LAI':   os.path.join(data_folder, 'LAI_2024.tif'),\n",
        "    'ALB':  os.path.join(data_folder, 'ALB_2024.tif'),\n",
        "    'ELV':   os.path.join(data_folder, 'ELV.tif'),\n",
        "    'SLP':   os.path.join(data_folder, 'SLP.tif'),\n",
        "    'DSR':   os.path.join(data_folder, 'DSR_2024.tif')\n",
        "})\n",
        "\n",
        "# Define file path for target variable (LST)\n",
        "lst_path = os.path.join(data_folder, 'LST_2024.tif')\n",
        "\n",
        "# Check that all files exist and print a warning if any are missing.\n",
        "print(\"Verifying file paths...\")\n",
        "for key, path in predictor_files.items():\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Warning: {key} file not found at {path}\")\n",
        "    else:\n",
        "        print(f\"{key} file found.\")\n",
        "if not os.path.exists(lst_path):\n",
        "    print(f\"Warning: LST file not found at {lst_path}\")\n",
        "else:\n",
        "    print(\"LST file found.\")\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "RYBFVyexiXbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Functions for Raster Operations and Visualizations**"
      ],
      "metadata": {
        "id": "HsZwQSp-vr8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "#  Define Functions for Raster Operations and Visualizations\n",
        "# =============================================================================\n",
        "def read_raster(raster_path):\n",
        "    \"\"\"\n",
        "    Reads a single-band raster file and returns its data and metadata.\n",
        "\n",
        "    Parameters:\n",
        "        raster_path (str): Path to the raster file.\n",
        "\n",
        "    Returns:\n",
        "        data (np.ndarray): 2D array of raster data.\n",
        "        meta (dict): Raster metadata (projection, transform, etc.).\n",
        "    \"\"\"\n",
        "    with rasterio.open(raster_path) as src:\n",
        "        data = src.read(1)  # Assuming a single-band raster.\n",
        "        meta = src.meta\n",
        "    return data, meta\n",
        "\n",
        "def plot_histogram(data, title, bins=50):\n",
        "    \"\"\"\n",
        "    Plots a histogram of the input data.\n",
        "\n",
        "    Parameters:\n",
        "        data (np.ndarray): 1D array of data values.\n",
        "        title (str): Title for the histogram.\n",
        "        bins (int): Number of bins in the histogram.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.hist(data[np.isfinite(data)], bins=bins, color='gray', edgecolor='black')\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Value\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_scatter(x, y, xlabel, ylabel, title):\n",
        "    \"\"\"\n",
        "    Plots a scatter plot of two variables.\n",
        "\n",
        "    Parameters:\n",
        "        x (np.ndarray): x-axis values.\n",
        "        y (np.ndarray): y-axis values.\n",
        "        xlabel (str): Label for x-axis.\n",
        "        ylabel (str): Label for y-axis.\n",
        "        title (str): Plot title.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.scatter(x, y, alpha=0.5, s=10)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "#  Example: Read a Raster and Display a Plot\n",
        "# =============================================================================\n",
        "# Read the NDVI raster as an example\n",
        "ndvi_path = predictor_files['DSR']\n",
        "ndvi_data, ndvi_meta = read_raster(ndvi_path)\n",
        "\n",
        "# Plot the histogram for NDVI data\n",
        "#plot_histogram(ndvi_data, \"Histogram of NDVI_2020\")"
      ],
      "metadata": {
        "id": "lsIMpqkNkYE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read All Predictor and Target Rasters**"
      ],
      "metadata": {
        "id": "SCYh6uTevu96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Read All Predictor and Target Rasters\n",
        "# =============================================================================\n",
        "print(\"Reading predictor rasters...\")\n",
        "predictors = {}\n",
        "meta = None\n",
        "for key, path in predictor_files.items():\n",
        "    data, meta = read_raster(path)\n",
        "    predictors[key] = data\n",
        "    print(f\"Read {key} with shape: {data.shape}\")\n",
        "\n",
        "print(\"\\nReading target raster (LST)...\")\n",
        "lst, _ = read_raster(lst_path)\n",
        "print(\"LST shape:\", lst.shape)\n",
        "print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "0kGOpZCwloDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "#  Check Consistency of Raster Shapes\n",
        "# =============================================================================\n",
        "shapes = [data.shape for data in predictors.values()] + [lst.shape]\n",
        "if len(set(shapes)) != 1:\n",
        "    print(\"Error: Raster files do not have the same dimensions. Please check your data.\")\n",
        "else:\n",
        "    print(\"All raster files have consistent dimensions.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7383h3wqmqIX",
        "outputId": "d4315d2b-7d43-4cf2-ccd1-0d74e9fe366e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All raster files have consistent dimensions.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Flatten Rasters and Stack Features**"
      ],
      "metadata": {
        "id": "xn_0OEVHvzwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Flatten Rasters and Stack Features\n",
        "# =============================================================================\n",
        "print(\"Preparing data for modeling...\")\n",
        "n_pixels = lst.size  # Total number of pixels (assumes all rasters share the same shape)\n",
        "\n",
        "# Flatten each predictor and store in a list following the order in predictor_files.\n",
        "feature_list = []\n",
        "for key in predictor_files.keys():\n",
        "    flat_array = predictors[key].flatten()\n",
        "    feature_list.append(flat_array)\n",
        "    # Plot histogram for each predictor (optional, for initial data inspection)\n",
        "    #plot_histogram(flat_array, f\"Histogram of {key}\")\n",
        "\n",
        "# Convert list of arrays to 2D feature array (rows: pixels, columns: features)\n",
        "features = np.vstack(feature_list).T\n",
        "\n",
        "# Flatten target variable\n",
        "lst_flat = lst.flatten()\n",
        "\n",
        "# =============================================================================\n",
        "# Create a Valid Data Mask (Filter Out Invalid Pixels)\n",
        "# =============================================================================\n",
        "# Here, valid data are those with finite values for all predictors and the target.\n",
        "valid_mask = np.isfinite(features).all(axis=1) & np.isfinite(lst_flat)\n",
        "print(\"Total pixels:\", n_pixels)\n",
        "print(\"Valid pixels for training:\", np.sum(valid_mask))\n",
        "print(\"\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# Filter Features and Target Data\n",
        "# =============================================================================\n",
        "features_valid = features[valid_mask]\n",
        "lst_valid      = lst_flat[valid_mask]\n",
        "\n",
        "# =============================================================================\n",
        "# Optional: Inspect Data Distribution for the Target\n",
        "# =============================================================================\n",
        "#plot_histogram(lst_valid, \"Histogram of LST (Valid Pixels)\")\n"
      ],
      "metadata": {
        "id": "ZKcqRPfQtXQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model** **Training**"
      ],
      "metadata": {
        "id": "eujN88PIv5oA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Set Up Random Forest Model Parameters (Using Tuned Hyperparameters)\n",
        "# =============================================================================\n",
        "n_estimators = 200\n",
        "max_features = 4\n",
        "min_samples_split = 2\n",
        "max_depth = None\n",
        "random_state = 42\n",
        "\n",
        "# Initialize and Train the Random Forest Regressor with Tuned Parameters\n",
        "# =============================================================================\n",
        "print(\"Initializing the Random Forest regressor with tuned hyperparameters...\")\n",
        "rf_model = RandomForestRegressor(n_estimators=n_estimators,\n",
        "                                 max_features=max_features,\n",
        "                                 min_samples_split=min_samples_split,\n",
        "                                 max_depth=max_depth,\n",
        "                                 oob_score=True,\n",
        "                                 random_state=random_state,\n",
        "                                 n_jobs=-1)\n",
        "\n",
        "print(\"Training the model...\")\n",
        "start_time = time.time()\n",
        "rf_model.fit(features_valid, lst_valid)\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Training completed in {elapsed_time:.2f} seconds.\\n\")\n"
      ],
      "metadata": {
        "id": "ifn4ZUg7bdwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate** **the** **Model**"
      ],
      "metadata": {
        "id": "77fbEttwwjfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Evaluate the Model Using Out-of-Bag Predictions\n",
        "# =============================================================================\n",
        "oob_score = rf_model.oob_score_\n",
        "print(\"OOB Score:\", oob_score)\n",
        "\n",
        "# OOB predictions for evaluation\n",
        "lst_oob_pred = rf_model.oob_prediction_\n",
        "rmse = np.sqrt(mean_squared_error(lst_valid, lst_oob_pred))\n",
        "mae = mean_absolute_error(lst_valid, lst_oob_pred)\n",
        "r2 = r2_score(lst_valid, lst_oob_pred)\n",
        "\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"MAE:\", mae)\n",
        "print(\"R2 Score:\", r2)\n",
        "print(\"\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# Save Evaluation Metrics to a CSV File\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "\n",
        "# Create a dictionary with evaluation metrics\n",
        "evaluation_metrics = {\n",
        "    'OOB_Score': [oob_score],\n",
        "    'RMSE': [rmse],\n",
        "    'MAE': [mae],\n",
        "    'R2': [r2]\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "df_eval = pd.DataFrame(evaluation_metrics)\n",
        "\n",
        "# Define the CSV output path (adjust data_folder as needed)\n",
        "csv_path = os.path.join(data_folder, 'evaluation_metrics.csv')\n",
        "\n",
        "# Save the DataFrame to CSV\n",
        "df_eval.to_csv(csv_path, index=False)\n",
        "print(\"Evaluation metrics saved to:\", csv_path)\n"
      ],
      "metadata": {
        "id": "WlS8AlTWw_zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scatter Plot: Observed vs.Predicted LST**"
      ],
      "metadata": {
        "id": "JFLguFIqxLBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Scatter Plot: Observed vs. OOB Predicted LST (for training pixels)\n",
        "# =============================================================================\n",
        "plot_scatter(lst_valid, lst_oob_pred, \"Observed LST\", \"OOB Predicted LST\",\n",
        "             \"Observed vs. OOB Predicted LST (Clipped)\")"
      ],
      "metadata": {
        "id": "lmVaLs1Kbgix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute Feature Importances**"
      ],
      "metadata": {
        "id": "HkPXxWrwxpzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Compute and Plot Feature Importances\n",
        "# =============================================================================\n",
        "importances = rf_model.feature_importances_\n",
        "feature_names = list(predictor_files.keys())\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(range(len(importances)), importances, tick_label=feature_names)\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.title(\"Random Forest Feature Importances\")\n",
        "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# Save Feature Importances to a CSV File\n",
        "# =============================================================================\n",
        "# Create a DataFrame with feature names and their corresponding importance values.\n",
        "df_features = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Define the CSV output path (adjust data_folder if necessary)\n",
        "csv_feature_importance = os.path.join(data_folder, 'feature_importances.csv')\n",
        "\n",
        "# Save the DataFrame to a CSV file without the index.\n",
        "df_features.to_csv(csv_feature_importance, index=False)\n",
        "print(\"Feature importances saved to:\", csv_feature_importance)\n"
      ],
      "metadata": {
        "id": "Aszbqi30xsEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predict LST for the Entire Image**"
      ],
      "metadata": {
        "id": "eAqW-SzCyQio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Predict LST for the Entire Image\n",
        "# =============================================================================\n",
        "print(\"Predicting LST for the entire image...\")\n",
        "predicted_lst = rf_model.predict(features)\n",
        "predicted_lst_image = predicted_lst.reshape(lst.shape)\n",
        "\n",
        "# =============================================================================\n",
        "#Handle Invalid Pixels in the Predicted Output\n",
        "# =============================================================================\n",
        "# For pixels that were originally invalid (e.g., nodata), assign a nodata value.\n",
        "predicted_lst_image[~np.isfinite(predictors['ELV'])] = np.nan\n",
        "\n",
        "# =============================================================================\n",
        "# Save the Predicted LST as a New GeoTIFF File\n",
        "# =============================================================================\n",
        "output_path = os.path.join(data_folder, 'predicted_LST.tif')\n",
        "meta.update(dtype=rasterio.float32, count=1)\n",
        "\n",
        "with rasterio.open(output_path, 'w', **meta) as dst:\n",
        "    dst.write(predicted_lst_image.astype(rasterio.float32), 1)\n",
        "print(\"Predicted LST image saved to:\", output_path)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "eOzW7TqmbU9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute the Difference_ \"e\" value**"
      ],
      "metadata": {
        "id": "1l1dRPttynQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# New Step: Compute and Save the Difference Image\n",
        "# =============================================================================\n",
        "# Compute the difference between the original LST and the predicted LST\n",
        "difference_image = lst - predicted_lst_image\n",
        "\n",
        "# Save the difference image as a new GeoTIFF file\n",
        "difference_output_path = os.path.join(data_folder, 'LST_e.tif')\n",
        "meta.update(dtype=rasterio.float32, count=1)\n",
        "\n",
        "with rasterio.open(difference_output_path, 'w', **meta) as dst:\n",
        "    dst.write(difference_image.astype(rasterio.float32), 1)\n",
        "print(\"Difference image (Original LST - Predicted LST) saved to:\", difference_output_path)\n",
        "print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "x9tAXSPstm8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization**"
      ],
      "metadata": {
        "id": "ymXxaknb000f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Visualize the Original, Predicted, and Difference Images Side by Side\n",
        "# =============================================================================\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 8))\n",
        "\n",
        "# Original LST image (clipped)\n",
        "cax1 = ax1.imshow(lst, cmap='viridis')\n",
        "ax1.set_title(\"Original LST\")\n",
        "ax1.axis('off')\n",
        "fig.colorbar(cax1, ax=ax1, fraction=0.046, pad=0.04)\n",
        "\n",
        "# Predicted LST image (clipped)\n",
        "cax2 = ax2.imshow(predicted_lst_image, cmap='viridis')\n",
        "ax2.set_title(\"Predicted LST\")\n",
        "ax2.axis('off')\n",
        "fig.colorbar(cax2, ax=ax2, fraction=0.046, pad=0.04)\n",
        "\n",
        "# Difference image (Original - Predicted)\n",
        "# Set the color limits symmetric around zero for better visualization of positive/negative errors.\n",
        "diff_abs_max = np.nanmax(np.abs(difference_image))\n",
        "cax3 = ax3.imshow(difference_image, cmap='RdBu', vmin=-diff_abs_max, vmax=diff_abs_max)\n",
        "ax3.set_title(\"Difference: e image\")\n",
        "ax3.axis('off')\n",
        "fig.colorbar(cax3, ax=ax3, fraction=0.046, pad=0.04)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KIaF8Tz00zHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predict LST Using New Data**"
      ],
      "metadata": {
        "id": "griAM3u51hHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# NEW: Predict LST Using New Data\n",
        "# =============================================================================\n",
        "print(\"Predicting LST using new data...\")\n",
        "\n",
        "# Define the new data folder and file paths for the new predictor data.\n",
        "new_data_folder = '/content/drive/MyDrive/your_new_data_folder'\n",
        "\n",
        "new_predictor_files = OrderedDict({\n",
        "    'elv':   os.path.join(new_data_folder, 'elv_new.tif'),\n",
        "    'slope': os.path.join(new_data_folder, 'slope_new.tif'),\n",
        "    'NDVI':  os.path.join(new_data_folder, 'NDVI_new.tif'),\n",
        "    'EVI':   os.path.join(new_data_folder, 'EVI_new.tif'),\n",
        "    'NDWI':  os.path.join(new_data_folder, 'NDWI_new.tif'),\n",
        "    'LAI':   os.path.join(new_data_folder, 'LAI_new.tif'),\n",
        "    'ALB':   os.path.join(new_data_folder, 'ALB_new.tif'),\n",
        "    'DSR':   os.path.join(new_data_folder, 'DSR_new.tif')\n",
        "})\n",
        "\n",
        "# =============================================================================\n",
        "# Read New Predictor Rasters\n",
        "# =============================================================================\n",
        "new_predictors = {}\n",
        "new_meta = None\n",
        "for key, path in new_predictor_files.items():\n",
        "    data, new_meta = read_raster(path)\n",
        "    new_predictors[key] = data\n",
        "    print(f\"Read new {key} with shape: {data.shape}\")\n",
        "\n",
        "# =============================================================================\n",
        "# Check Consistency of New Raster Shapes\n",
        "# =============================================================================\n",
        "new_shapes = [data.shape for data in new_predictors.values()]\n",
        "if len(set(new_shapes)) != 1:\n",
        "    print(\"Error: New raster files do not have the same dimensions. Please check your data.\")\n",
        "else:\n",
        "    print(\"All new raster files have consistent dimensions.\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "rrFhBbpXt_KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predict LST for the New Data Using the Trained Model**"
      ],
      "metadata": {
        "id": "77awtLPv2pox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Flatten New Rasters and Stack Features\n",
        "# =============================================================================\n",
        "new_feature_list = []\n",
        "for key in new_predictor_files.keys():\n",
        "    flat_array = new_predictors[key].flatten()\n",
        "    new_feature_list.append(flat_array)\n",
        "# Create a 2D array where each row represents a pixel and each column a predictor.\n",
        "new_features = np.vstack(new_feature_list).T\n",
        "\n",
        "# =============================================================================\n",
        "# Predict LST for the New Data Using the Trained Model\n",
        "# =============================================================================\n",
        "new_predicted_lst = rf_model.predict(new_features)\n",
        "# Reshape the 1D predictions back into the 2D image shape (assumed identical for all predictors)\n",
        "new_predicted_lst_image = new_predicted_lst.reshape(new_predictors[list(new_predictor_files.keys())[0]].shape)\n",
        "\n",
        "# =============================================================================\n",
        "# Handle Invalid Pixels in the New Prediction\n",
        "# =============================================================================\n",
        "# For example, if the new elevation data has invalid pixels, mark these as NaN in the prediction.\n",
        "new_predicted_lst_image[~np.isfinite(new_predictors['elv'])] = np.nan\n",
        "\n",
        "# =============================================================================\n",
        "# Save the New Predicted LST as a GeoTIFF File\n",
        "# =============================================================================\n",
        "new_output_path = os.path.join(new_data_folder, 'predicted_LST_new.tif')\n",
        "new_meta.update(dtype=rasterio.float32, count=1)\n",
        "\n",
        "with rasterio.open(new_output_path, 'w', **new_meta) as dst:\n",
        "    dst.write(new_predicted_lst_image.astype(rasterio.float32), 1)\n",
        "print(\"New predicted LST image saved to:\", new_output_path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b5xDCyTJ1284"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualize the New Predicted LST Image**"
      ],
      "metadata": {
        "id": "fZRwqa5q3SL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Visualize the New Predicted LST Image\n",
        "# =============================================================================\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(new_predicted_lst_image, cmap='viridis')\n",
        "plt.title(\"Predicted LST (New Data)\")\n",
        "plt.axis('off')\n",
        "plt.colorbar(label='LST Value')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xa-s06OI3Oon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Normalized LST**"
      ],
      "metadata": {
        "id": "iBymuulh3y6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"new_predicted_lst_image\" is the prediction from new data.\n",
        "adjusted_new_pred = new_predicted_lst_image + difference_image\n",
        "\n",
        "# =============================================================================\n",
        "# Save the Adjusted New Prediction as a New GeoTIFF File\n",
        "# =============================================================================\n",
        "adjusted_output_path = os.path.join(new_data_folder, 'Normalized LST.tif')\n",
        "new_meta.update(dtype=rasterio.float32, count=1)\n",
        "with rasterio.open(adjusted_output_path, 'w', **new_meta) as dst:\n",
        "    dst.write(adjusted_new_pred.astype(rasterio.float32), 1)\n",
        "print(\"Normalized LST image saved to:\", adjusted_output_path)\n",
        "\n",
        "# =============================================================================\n",
        "# Plot the Adjusted New Prediction\n",
        "# =============================================================================\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(adjusted_new_pred, cmap='viridis')\n",
        "plt.title(\"Adjusted New Predicted LST\")\n",
        "plt.axis('off')\n",
        "plt.colorbar(label='LST Value')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "8uR99SgL76t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute Evaluation Metrics for final normalization**"
      ],
      "metadata": {
        "id": "CCBzAQ6EA3ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Evaluate Adjusted New Prediction Accuracy Across the Entire Image\n",
        "# =============================================================================\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Flatten both arrays for metric calculation\n",
        "true_lst_flat = lst.flatten()\n",
        "adjusted_pred_flat = adjusted_new_pred.flatten()\n",
        "\n",
        "# Mask out invalid pixels (where ground truth or adjusted prediction is invalid)\n",
        "valid_mask = np.isfinite(true_lst_flat) & np.isfinite(adjusted_pred_flat)\n",
        "\n",
        "# Apply mask\n",
        "y_true = true_lst_flat[valid_mask]\n",
        "y_pred = adjusted_pred_flat[valid_mask]\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "r2_adj = r2_score(y_true, y_pred)\n",
        "rmse_adj = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "mae_adj = mean_absolute_error(y_true, y_pred)\n",
        "pearson_corr, p_value = pearsonr(y_true, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Adjusted New Prediction Evaluation Metrics (Valid Pixels Only):\")\n",
        "print(\"Pearson Correlation:\", pearson_corr)\n",
        "print(\"p-value:\", p_value)\n",
        "print(\"R² Score:\", r2_adj)\n",
        "print(\"RMSE:\", rmse_adj)\n",
        "print(\"MAE:\", mae_adj)\n",
        "\n",
        "# =============================================================================\n",
        "# 20. Save Adjusted Prediction Metrics to CSV\n",
        "# =============================================================================\n",
        "\n",
        "# Define output file path\n",
        "csv_output_path = os.path.join(new_data_folder, 'adjusted_prediction_metrics.csv')\n",
        "\n",
        "# Create a DataFrame with the metrics\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Pearson_Correlation': [pearson_corr],\n",
        "    'p_value': [p_value],\n",
        "    'R2_Score': [r2_adj],\n",
        "    'RMSE': [rmse_adj],\n",
        "    'MAE': [mae_adj]\n",
        "})\n",
        "\n",
        "# Save the DataFrame to CSV\n",
        "metrics_df.to_csv(csv_output_path, index=False)\n",
        "\n",
        "print(\"Adjusted prediction evaluation metrics saved to:\", csv_output_path)\n"
      ],
      "metadata": {
        "id": "1LACsoryC5hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**correlation Matrix Example **"
      ],
      "metadata": {
        "id": "rJXa1cX0BJbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# =============================================================================\n",
        "#  Evaluate Prediction Accuracy Across the Entire Image\n",
        "# =============================================================================\n",
        "\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Flatten both arrays for metric calculation\n",
        "true_lst_flat = lst.flatten()\n",
        "predicted_lst_flat = predicted_lst_image.flatten()\n",
        "\n",
        "# Mask out invalid pixels (where elevation or LST was invalid)\n",
        "valid_mask = np.isfinite(true_lst_flat) & np.isfinite(predicted_lst_flat)\n",
        "\n",
        "# Apply mask\n",
        "y_true = true_lst_flat[valid_mask]\n",
        "y_pred = predicted_lst_flat[valid_mask]\n",
        "\n",
        "# Calculate metrics\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Full Image Evaluation Metrics (Valid Pixels Only):\")\n",
        "print(\"R² Score:\", r2)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"MAE:\", mae)\n",
        "\n",
        "# =============================================================================\n",
        "# Save Metrics to CSV\n",
        "# =============================================================================\n",
        "\n",
        "# Define output file path\n",
        "csv_output_path = os.path.join(data_folder, 'LST_evaluation_metrics.csv')\n",
        "\n",
        "# Create a DataFrame with the metrics\n",
        "metrics_df = pd.DataFrame({\n",
        "    'R2_Score': [r2],\n",
        "    'RMSE': [rmse],\n",
        "    'MAE': [mae]\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "metrics_df.to_csv(csv_output_path, index=False)\n",
        "\n",
        "print(\"Evaluation metrics saved to:\", csv_output_path)\n"
      ],
      "metadata": {
        "id": "9CDPVqHQq3r0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d938ef4-6c61-4f6c-bacb-955f39bb8db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Image Evaluation Metrics (Valid Pixels Only):\n",
            "R² Score: 0.9900307302644273\n",
            "RMSE: 0.4372856628712171\n",
            "MAE: 0.26704630066368046\n",
            "Evaluation metrics saved to: /content/drive/MyDrive/Taiwan_Yearly /Taiwan_2024/LST_evaluation_metrics.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation plot"
      ],
      "metadata": {
        "id": "SUGvKXOssz-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_scatter(true_values, predicted_values, xlabel, ylabel, title, doy=None):\n",
        "    \"\"\"\n",
        "    Create a scatter density plot of observed vs. predicted LST with fixed axis limits.\n",
        "\n",
        "    Parameters:\n",
        "    - true_values: Array of original LST values\n",
        "    - predicted_values: Array of predicted LST values (e.g., OOB or full image)\n",
        "    - xlabel: Label for the X-axis\n",
        "    - ylabel: Label for the Y-axis\n",
        "    - title: Plot title\n",
        "    - doy: Day of Year (optional, for labeling)\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "\n",
        "    # Create a density scatter plot using hexbin\n",
        "    hb = plt.hexbin(true_values, predicted_values, gridsize=70, cmap=\"jet\", bins=\"log\", mincnt=1)\n",
        "\n",
        "    # Colorbar to show density levels\n",
        "    cb = plt.colorbar(hb)\n",
        "    cb.set_label('Count')\n",
        "\n",
        "    # Plot 1:1 reference line from (270, 270) to (310, 310)\n",
        "    plt.plot([275, 310], [275, 310], 'r-', linewidth=1)\n",
        "\n",
        "    # Set axis limits\n",
        "    plt.xlim(275, 310)\n",
        "    plt.ylim(275, 310)\n",
        "\n",
        "    # Compute performance metrics\n",
        "    r2 = np.corrcoef(true_values, predicted_values)[0, 1] ** 2\n",
        "    rmse = np.sqrt(np.mean((true_values - predicted_values) ** 2))\n",
        "\n",
        "    # Add text for R² and RMSE in the upper-left corner\n",
        "    plt.text(285, 302, f'R²={r2:.2f}\\nRMSE={rmse:.2f}K', fontsize=12, color='black')\n",
        "\n",
        "    # Labels and Title\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    if doy:\n",
        "        plt.title(f\"{title}\\nDOY {doy}\")\n",
        "    else:\n",
        "        plt.title(title)\n",
        "\n",
        "    plt.show()\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_scatter(true_values, predicted_values, xlabel, ylabel, title, doy=None):\n",
        "    \"\"\"\n",
        "    Create a scatter density plot of observed vs. predicted LST with fixed axis limits.\n",
        "\n",
        "    Parameters:\n",
        "    - true_values: Array of original LST values\n",
        "    - predicted_values: Array of predicted LST values (e.g., OOB or full image)\n",
        "    - xlabel: Label for the X-axis\n",
        "    - ylabel: Label for the Y-axis\n",
        "    - title: Plot title\n",
        "    - doy: Day of Year (optional, for labeling)\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "\n",
        "    # Create a density scatter plot using hexbin\n",
        "    hb = plt.hexbin(true_values, predicted_values, gridsize=70, cmap=\"jet\", bins=\"log\", mincnt=1)\n",
        "\n",
        "    # Colorbar to show density levels\n",
        "    cb = plt.colorbar(hb)\n",
        "    cb.set_label('Count')\n",
        "\n",
        "    # Plot 1:1 reference line from (270, 270) to (310, 310)\n",
        "    plt.plot([280, 310], [280, 310], 'r-', linewidth=1)\n",
        "\n",
        "    # Set axis limits\n",
        "    plt.xlim(280, 310)\n",
        "    plt.ylim(280, 310)\n",
        "\n",
        "    # Compute performance metrics\n",
        "    r2 = np.corrcoef(true_values, predicted_values)[0, 1] ** 2\n",
        "    rmse = np.sqrt(np.mean((true_values - predicted_values) ** 2))\n",
        "\n",
        "    # Add text for R² and RMSE in the upper-left corner\n",
        "    plt.text(282, 306, f'R²={r2:.2f}\\nRMSE={rmse:.2f}K', fontsize=12, color='black')\n",
        "\n",
        "    # Labels and Title\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    #if doy:\n",
        "     #   plt.title(f\"{title}\\nDOY {doy}\")\n",
        "   # else:\n",
        "    #    plt.title(title)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example Usage:\n",
        "plot_scatter(lst_valid, lst_oob_pred, \"Original LST (K)\", \"Estimated LST (K)\",\"Correlation\", doy=48)"
      ],
      "metadata": {
        "id": "3_R5JxpKsvca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Considerations**"
      ],
      "metadata": {
        "id": "VH9KkEFaBdce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =============================================================================\n",
        "#  Final Considerations and Next Steps\n",
        "# =============================================================================\n",
        "# In this script, we critically examined the following:\n",
        "# - Data consistency: ensuring all GeoTIFFs have the same dimensions.\n",
        "# - Data filtering: removing pixels with non-finite values.\n",
        "# - Model evaluation: using OOB score, RMSE, MAE, and R2.\n",
        "# - Visualization: comparing original vs. predicted LST and analyzing feature importances.\n",
        "#\n",
        "# Further steps you may consider:\n",
        "# - Cross-validation with a separate test set.\n",
        "# - Advanced hyperparameter tuning (e.g., grid search).\n",
        "# - More robust handling of nodata values and spatial outliers.\n",
        "# - Exploring additional predictors or ensemble methods.\n"
      ],
      "metadata": {
        "id": "igAnkkcKVgtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Cross-validation with a Separate Test Set\n",
        "# =============================================================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split valid data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_valid, lst_valid,\n",
        "                                                    test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Data split into training and test sets.\")\n",
        "print(\"Training set size:\", X_train.shape[0])\n",
        "print(\"Test set size:\", X_test.shape[0])\n",
        "print(\"\\n\")\n",
        "\n",
        "# Train a Random Forest model on the training set (without using OOB here)\n",
        "rf_cv_model = RandomForestRegressor(n_estimators=n_estimators,\n",
        "                                    max_features=max_features,\n",
        "                                    random_state=random_state,\n",
        "                                    n_jobs=-1)\n",
        "print(\"Training the model on the training set...\")\n",
        "rf_cv_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_test_pred = rf_cv_model.predict(X_test)\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
        "r2_test = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"Test Set Evaluation Metrics:\")\n",
        "print(\"RMSE:\", rmse_test)\n",
        "print(\"MAE:\", mae_test)\n",
        "print(\"R2 Score:\", r2_test)\n",
        "print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "DOHXDm1DczM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01871910-dab3-4a2b-e399-352545254d2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data split into training and test sets.\n",
            "Training set size: 26876\n",
            "Test set size: 6720\n",
            "\n",
            "\n",
            "Training the model on the training set...\n",
            "Test Set Evaluation Metrics:\n",
            "RMSE: 0.7823354287657159\n",
            "MAE: 0.5965739514146434\n",
            "R2 Score: 0.9671603613701477\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}